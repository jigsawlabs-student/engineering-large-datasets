{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DateTimes and Handling Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, we now have have our data properly coerced.  We almost can train our model, but doing so involves handling both our datetime and na values.  We'll do so in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = \"./dtypes_coerced_cats.json\"\n",
    "with open(file, 'r') as f:\n",
    "    dtypes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coerced_df = pd.read_csv('./coerced_cats.csv', index_col = 0).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>extra_people</th>\n",
       "      <th>id</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>last_scraped</th>\n",
       "      <th>name</th>\n",
       "      <th>summary</th>\n",
       "      <th>space</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_x0_12047</th>\n",
       "      <th>zipcode_x0_12049</th>\n",
       "      <th>zipcode_x0_12051</th>\n",
       "      <th>zipcode_x0_12053</th>\n",
       "      <th>zipcode_x0_12055</th>\n",
       "      <th>zipcode_x0_12059</th>\n",
       "      <th>zipcode_x0_12435</th>\n",
       "      <th>zipcode_x0_13353</th>\n",
       "      <th>zipcode_x0_13357</th>\n",
       "      <th>zipcode_x0_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://www.airbnb.com/rooms/2015</td>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>Berlin-Mitte Value! Quiet courtyard/very central</td>\n",
       "      <td>Great location!  30 of 75 sq meters. This wood...</td>\n",
       "      <td>A+++ location! This „Einliegerwohnung“ is an e...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2695</td>\n",
       "      <td>https://www.airbnb.com/rooms/2695</td>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>Prenzlauer Berg close to Mauerpark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In the summertime we are spending most of our ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_response_rate  security_deposit  cleaning_fee  extra_people    id  \\\n",
       "0                96.0             200.0          30.0          28.0  2015   \n",
       "1                 NaN               0.0           0.0           0.0  2695   \n",
       "\n",
       "                         listing_url last_scraped  \\\n",
       "0  https://www.airbnb.com/rooms/2015   2018-11-07   \n",
       "1  https://www.airbnb.com/rooms/2695   2018-11-07   \n",
       "\n",
       "                                               name  \\\n",
       "0  Berlin-Mitte Value! Quiet courtyard/very central   \n",
       "1                Prenzlauer Berg close to Mauerpark   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Great location!  30 of 75 sq meters. This wood...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                               space  ... zipcode_x0_12047  \\\n",
       "0  A+++ location! This „Einliegerwohnung“ is an e...  ...              0.0   \n",
       "1  In the summertime we are spending most of our ...  ...              0.0   \n",
       "\n",
       "  zipcode_x0_12049 zipcode_x0_12051 zipcode_x0_12053 zipcode_x0_12055  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "\n",
       "  zipcode_x0_12059 zipcode_x0_12435 zipcode_x0_13353  zipcode_x0_13357  \\\n",
       "0              0.0              0.0              0.0               0.0   \n",
       "1              0.0              0.0              0.0               0.0   \n",
       "\n",
       "  zipcode_x0_other  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "\n",
       "[2 rows x 247 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerced_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = coerced_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 19)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.select_dtypes('object').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_df = updated_df.select_dtypes(exclude = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 228)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing DateTimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with coercing our datetimes.  To do so, we'll use our `add_datepart` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def add_datepart(df, fldname, drop=True, time=False, errors=\"raise\"):\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's select all of the datetime columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df = coerced_df.select_dtypes('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['last_scraped', 'host_since', 'calendar_last_scraped', 'first_review',\n",
       "       'last_review'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use list iteration to call `add_datepart` on each of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ignore the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[add_datepart(datetime_df, col) for col in datetime_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing so, we should have 65 columns in our `datetime_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 65)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop our datetime columns from the `coerced_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_dt_df = coerced_df.drop(columns = ['last_scraped', 'host_since', 'calendar_last_scraped', 'first_review',\n",
    "       'last_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add in the new datetime columns to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_dt_df[datetime_df.columns] = datetime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 288)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_dt_df.shape\n",
    "# (8000, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is Null Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's use the DataFrameMapper to impute the missing values and add a `is_missing` for each column with missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find the names of the columns with any `na` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_na_cols = replaced_dt_df.isna().any(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_na = has_na_cols[has_na_cols == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_na.shape\n",
    "# (35,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['host_response_rate', 'security_deposit', 'cleaning_fee',\n",
       "       'host_listings_count', 'host_total_listings_count', 'bathrooms',\n",
       "       'bedrooms', 'beds', 'square_feet', 'review_scores_rating',\n",
       "       'review_scores_accuracy', 'review_scores_cleanliness',\n",
       "       'review_scores_checkin', 'review_scores_communication',\n",
       "       'review_scores_location', 'review_scores_value', 'reviews_per_month',\n",
       "       'host_sinceYear', 'host_sinceMonth', 'host_sinceWeek', 'host_sinceDay',\n",
       "       'host_sinceDayofweek', 'host_sinceDayofyear', 'first_reviewYear',\n",
       "       'first_reviewMonth', 'first_reviewWeek', 'first_reviewDay',\n",
       "       'first_reviewDayofweek', 'first_reviewDayofyear', 'last_reviewYear',\n",
       "       'last_reviewMonth', 'last_reviewWeek', 'last_reviewDay',\n",
       "       'last_reviewDayofweek', 'last_reviewDayofyear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_na\n",
    "# ['host_response_rate', 'security_deposit', 'cleaning_fee',\n",
    "#        'host_listings_count', 'host_total_listings_count', 'bathrooms',\n",
    "#        'bedrooms', 'beds', 'square_feet', 'review_scores_rating',\n",
    "#        'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "#        'review_scores_checkin', 'review_scores_communication',\n",
    "#        'review_scores_location', 'review_scores_value', 'reviews_per_month',\n",
    "#        'host_sinceYear', 'host_sinceMonth', 'host_sinceWeek', 'host_sinceDay',\n",
    "#        'host_sinceDayofweek', 'host_sinceDayofyear', 'first_reviewYear',\n",
    "#        'first_reviewMonth', 'first_reviewWeek', 'first_reviewDay',\n",
    "#        'first_reviewDayofweek', 'first_reviewDayofyear', 'last_reviewYear',\n",
    "#        'last_reviewMonth', 'last_reviewWeek', 'last_reviewDay',\n",
    "#        'last_reviewDayofweek', 'last_reviewDayofyear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets use list iteration to create steps that perform a simple impute on each of our above columns, and that add an column that indicates if the value is missing, for each of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column should have an `is_na` column should have a column name of the form `col_name_is_na`, for example `review_scores_rating_is_na`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "imputer_steps = [([col], SimpleImputer()) for col in cols_with_na]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_missing_steps = [([col], MissingIndicator(), {'alias': f'{col}_is_na'})\n",
    "                    for col in cols_with_na]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we can combine the steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_steps = imputer_steps + is_missing_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_null_mapper = DataFrameMapper(combined_steps, df_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed_df = is_null_mapper.fit_transform(replaced_dt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>host_total_listings_count</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>first_reviewWeek_is_na</th>\n",
       "      <th>first_reviewDay_is_na</th>\n",
       "      <th>first_reviewDayofweek_is_na</th>\n",
       "      <th>first_reviewDayofyear_is_na</th>\n",
       "      <th>last_reviewYear_is_na</th>\n",
       "      <th>last_reviewMonth_is_na</th>\n",
       "      <th>last_reviewWeek_is_na</th>\n",
       "      <th>last_reviewDay_is_na</th>\n",
       "      <th>last_reviewDayofweek_is_na</th>\n",
       "      <th>last_reviewDayofyear_is_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>465.872727</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91.418246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>465.872727</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_response_rate  security_deposit  cleaning_fee  host_listings_count  \\\n",
       "0           96.000000             200.0          30.0                  4.0   \n",
       "1           91.418246               0.0           0.0                  1.0   \n",
       "\n",
       "   host_total_listings_count  bathrooms  bedrooms  beds  square_feet  \\\n",
       "0                        4.0        1.0       1.0   2.0   465.872727   \n",
       "1                        1.0        1.0       1.0   1.0   465.872727   \n",
       "\n",
       "   review_scores_rating  ...  first_reviewWeek_is_na  first_reviewDay_is_na  \\\n",
       "0                  93.0  ...                   False                  False   \n",
       "1                 100.0  ...                   False                  False   \n",
       "\n",
       "   first_reviewDayofweek_is_na  first_reviewDayofyear_is_na  \\\n",
       "0                        False                        False   \n",
       "1                        False                        False   \n",
       "\n",
       "   last_reviewYear_is_na  last_reviewMonth_is_na  last_reviewWeek_is_na  \\\n",
       "0                  False                   False                  False   \n",
       "1                  False                   False                  False   \n",
       "\n",
       "   last_reviewDay_is_na  last_reviewDayofweek_is_na  \\\n",
       "0                 False                       False   \n",
       "1                 False                       False   \n",
       "\n",
       "   last_reviewDayofyear_is_na  \n",
       "0                       False  \n",
       "1                       False  \n",
       "\n",
       "[2 rows x 70 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_transformed_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling our mapper, we should not have any columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_transformed_df.isna().any(axis = 0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can drop our original columns with na values.  And replace them with our columns from the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_is_na = replaced_dt_df.drop(columns = cols_with_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed_df[df_with_is_na.columns] = df_with_is_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_is_na.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed_df.to_csv('cleaned_listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we finished coercing our data.  We did so by both converting our datetime columns and by  removing our null values.  For the datetime columns, we selected our datetime columns and then looped through them, using our `add_datepart` function.  For the columns with na values, we used a DataFrameMapper to both impute the missing values, and add a corresponding `is_na` column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
